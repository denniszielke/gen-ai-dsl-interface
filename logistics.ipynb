{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning logistics for shipping goods from one place to another\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit==1.35.0 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (1.35.0)\n",
      "Requirement already satisfied: langchain==0.2.5 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: azure-identity==1.16.1 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.16.1)\n",
      "Requirement already satisfied: langchain-openai==0.1.8 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.1.8)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: langchainhub==0.1.20 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.1.20)\n",
      "Requirement already satisfied: pandas==2.2.2 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
      "Requirement already satisfied: openai==1.34.0 in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (1.34.0)\n",
      "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.2.5)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement math (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for math\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# from langchain_core.tools import tools\n",
    "# from langchain import agents\n",
    "# from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "\n",
    "def time_for_loading(weight):\n",
    "    print(\"calculate time for loading for weight: \", weight)\n",
    "    if weight <= 2:\n",
    "        return 1\n",
    "    elif weight <= 6:\n",
    "        return 3\n",
    "    elif weight <= 10:\n",
    "        return 4\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "def calculate_travel_time(weight, distance):\n",
    "    print(\"calculate time for traveling for weight: \", weight, \" and distance: \", distance)\n",
    "    if weight <= 2:\n",
    "        return 1.5 * distance\n",
    "    elif weight <= 6:\n",
    "        return 3 * distance\n",
    "    elif weight <= 10:\n",
    "        return 4 * distance\n",
    "    else:\n",
    "        return 10 * distance\n",
    "    \n",
    "import pytz\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_time(location):\n",
    "    try:\n",
    "        location = str.replace(location, \" \", \"\")\n",
    "        location = str.replace(location, \"\\\"\", \"\")\n",
    "        # Get the timezone for the city\n",
    "        timezone = pytz.timezone(location)\n",
    "\n",
    "        # Get the current time in the timezone\n",
    "        now = datetime.now(timezone)\n",
    "        current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "\n",
    "        return current_time\n",
    "    except:\n",
    "        return \"Sorry, I couldn't find the timezone for that location.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07:54:21 PM'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_time('Europe/Berlin\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_time\",\n",
    "                \"description\": \"Get the current time in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The location name. The pytz is used to get the timezone for that location. Location names should be in a format like America/New_York, Asia/Bangkok, Europe/London\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_travel_time\",\n",
    "                \"description\": \"Calculates the time in minutes for for a truck to travel a given distance in kilometers\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"weight\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"The total weight of the cargo on the truck in kilograms\"\n",
    "                        },\n",
    "                        \"distance\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"The distance a truck has to travel in kilometers\"\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"weight\", \"distance\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "available_functions = {\n",
    "            \"get_current_time\": get_current_time,\n",
    "            \"calculate_travel_time\": calculate_travel_time,\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Azure OpenAI Endpoint: https://dzgpt4westus.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if load_dotenv():\n",
    "    print(\"Found Azure OpenAI Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "    # Create an instance of Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "        openai_api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "        temperature=0,\n",
    "        streaming=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "commandprompt = '''\n",
    "    ##\n",
    "    You are a logistic agent for calculating time for shipping cargo of boxes using a truck. \n",
    "    You need to perform the following tasks based on the User query. \n",
    "    The task aims to create commands and provide the commands as output.\n",
    "    Only one box can be loaded or unloaded at a time.\n",
    "    You should calculate the weight of the truck before every time a box has to be loaded to make sure you do no exceed the maximum weight.\n",
    "    It is most important to not exceed the maximum weight of the truck or the maximum number of boxes that can be loaded onto the truck. Distribute the boxed accordingly and load the heaviest boxes first.\n",
    "\n",
    "    If you are not able to understand the User query. Take a deep breath, think step by step. \n",
    "    Despite deliberation, if you are not able to create commands. Just answer with not able to create commands.\n",
    "    The grammar defines several commands for shipping cargo. Each command takes specific arguments. \n",
    "    The '%Y-%m-%d %H:%M:%S' means string formatted datetime format.\n",
    "\n",
    "    A blue box weighs 5 kilograms and has dimensions. A red box weighs 10 kilograms. A green box weighs 15 kilograms.\n",
    "    A truck can carry a maximum of 5 boxes of cargo or load a maximum of 50 kilograms.\n",
    "    It takes 3 minutes to load or unload a box onto a truck. Only a single box can be loaded or unloaded at a time.\n",
    "    \n",
    "    Use the tool calculate_travel_time to calculate the traveling time of a truck. The function takes the weight of the cargo in kilograms and the distance in kilometers as arguments.\n",
    "\n",
    "    You are able to create the follwing commands:\n",
    "\n",
    "    The `prepare_truck` command takes new truck and gives it an unique identifier.\n",
    "        `prepare_truck (truck_id)`\n",
    "    \n",
    "    The `load_box_on_truck` command takes a weight of the box as argument and adds a box to the truck. The weight is a number that represents the weight of the cargo in kilograms.\n",
    "        `load_box_on_truck (truck_id, box_id, weight)`\n",
    "\n",
    "    The `calculate_weight_of_truck` command calculated the weight of all the boxes in the truck. The weight is a number that represents the weight of the cargo in kilograms.\n",
    "        `calculate_weight_of_truck (truck_id)`\n",
    "\n",
    "    The `drive_truck_to_location` command takes a weight of the cargo in kilograms and the distance in kilometers. The weight is a number that represents the weight of the cargo in kilograms.\n",
    "        `drive_truck_to_location (truck_id, weight, distance)`\n",
    "    \n",
    "    The `unload_box_from_truck` command takes a weight of the box as argument and unloads a box from the truck. The weight is a number that represents the weight of the cargo in kilograms.\n",
    "        `unload_box_from_truck (truck_id, box_id, weight)`\n",
    "\n",
    "    ## Here are some examples of user inputs that you can use to generate the commands defined by the grammar:\n",
    "\n",
    "    1. For preparing a truck:\n",
    "    \"Please prepare a truck with ID 42.\"\n",
    "\n",
    "    2. For loading a box on a truck:\n",
    "    \"Please load a blue box with ID 123 on the truck with ID 42.\"\n",
    "    \"Please load a red box with ID 43 on the truck with ID 42.\"\n",
    "    \"Please load a red box with ID 44 on the truck with ID 42.\"\n",
    "\n",
    "    3. For calculating the weight of the truck:\n",
    "    \"Please calculate the weight of the truck with ID 42 after loading the boxes.\"\n",
    "    \n",
    "    4. For driving a truck to a location:\n",
    "    \"Please drive truck with ID 42 to the location 100 kilometers away.\"\n",
    "\n",
    "    5. For unloading a box from a truck:\n",
    "    \"Please unload the blue box with ID 123 from the truck with ID 42.\"\n",
    "\n",
    "    Remember to replace the weights, distance, dates, times, and IDs with your actual data. Also update the weight of the truck after every time a box has been loaded or unloaded.\n",
    "    The dates and times should be in the format '%Y-%m-%d %H:%M:%S'.\n",
    "\n",
    "    ## Here are some examples of how the output might look like based on the functions you provided:\n",
    "\n",
    "    1. For preparing a truck:\n",
    "    `prepare_truck(\"42\")`\n",
    "\n",
    "    2. For loading a box on a truck:\n",
    "    `load_box_on_truck(\"42\", \"123\", 5)`\n",
    "    `load_box_on_truck(\"42\", \"43\", 10)`\n",
    "    `load_box_on_truck(\"42\", \"44\", 10)`\n",
    "\n",
    "    3. For calculating the weight of the truck:\n",
    "    `calculate_weight_of_truck(\"42\")`\n",
    "\n",
    "    4. For driving a truck to a location:\n",
    "    `drive_truck_to_location(\"42\", 25, 100)`\n",
    "\n",
    "    5. For unloading a box from a truck:\n",
    "    `unload_box_from_truck(\"42\", \"123\", 5)`\n",
    "\n",
    "    ## Your response ought to be the command only as follows examples. However, you can prompt for input to provide the command parameters.\n",
    "\n",
    "    1. `prepare_truck(\"42\")`\n",
    "    2. `load_box_on_truck(\"42\", \"123\", 5)`\n",
    "    3. `calculate_weight_of_truck(\"42\")`\n",
    "    4. `drive_truck_to_location(\"42\", 25, 100)`\n",
    "    5. `unload_box_from_truck(\"42\", \"123\", 5)`\n",
    "\n",
    "    ##\n",
    "\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"commandprompt\"],\n",
    "    template=\"{commandprompt} User input is: {input}?\",\n",
    ")\n",
    "# Create a chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`prepare_truck(\"1\")`\n",
      "`load_box_on_truck(\"1\", \"1\", 15)`\n",
      "`load_box_on_truck(\"1\", \"2\", 15)`\n",
      "`load_box_on_truck(\"1\", \"3\", 15)`\n",
      "`load_box_on_truck(\"1\", \"4\", 5)`\n",
      "`calculate_weight_of_truck(\"1\")`\n",
      "\n",
      "`prepare_truck(\"2\")`\n",
      "`load_box_on_truck(\"2\", \"5\", 5)`\n",
      "`load_box_on_truck(\"2\", \"6\", 5)`\n",
      "`load_box_on_truck(\"2\", \"7\", 10)`\n",
      "`load_box_on_truck(\"2\", \"8\", 15)`\n",
      "`calculate_weight_of_truck(\"2\")`\n",
      "\n",
      "`prepare_truck(\"3\")`\n",
      "`load_box_on_truck(\"3\", \"9\", 15)`\n",
      "`calculate_weight_of_truck(\"3\")`\n"
     ]
    }
   ],
   "source": [
    "# Run the chain only specifying the input variable.\n",
    "response = chain.invoke({\"input\": \"I have a red box and 3 blue boxes and 4 green boxes. Please prepare enough trucks and load the boxes. Please calculate the weight for each truck.\", \"commandprompt\": commandprompt})\n",
    "\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "To handle this request, we will need to prepare trucks and load the boxes while ensuring that we do not exceed the maximum weight or box count for each truck. Since we don't have specific IDs for each box, we'll use hypothetical IDs for the boxes. We'll calculate the weight of each truck after loading the boxes.\n",
    "\n",
    "First, let's start by preparing the needed trucks and loading the boxes as per the weights provided:\n",
    "\n",
    "1. Red box = 10 kilograms\n",
    "2. Blue box = 5 kilograms each, 3 blue boxes = 3 * 5 = 15 kilograms\n",
    "3. Green box = 15 kilograms each, 4 green boxes = 4 * 15 = 60 kilograms\n",
    "\n",
    "Total weight of all boxes = 10 (red) + 15 (blue) + 60 (green) = 85 kilograms\n",
    "\n",
    "Since a truck can only carry a maximum of 50 kilograms, we will need two trucks to carry all the boxes.\n",
    "\n",
    "### Preparing trucks and loading boxes (using hypothetical box IDs for illustration):\n",
    "\n",
    "#### Truck 1:\n",
    "- Prepare Truck 1 with ID 101\n",
    "- Load 1 Red Box (ID: R1), weight = 10\n",
    "- Load 3 Blue Boxes (IDs: B1, B2, B3), weight = 15 (3 * 5)\n",
    "- The truck is now at 25 kilograms, so we can load 1 Green Box (ID: G1), weight = 15\n",
    "\n",
    "#### Truck 2:\n",
    "- Prepare Truck 2 with ID 102\n",
    "- Load 3 Green Boxes (IDs: G2, G3, G4), weight = 45 (3 * 15)\n",
    "\n",
    "Now we'll generate the commands:\n",
    "\n",
    "1. `prepare_truck(\"101\")`\n",
    "2. `load_box_on_truck(\"101\", \"R1\", 10)`\n",
    "3. `load_box_on_truck(\"101\", \"B1\", 5)`\n",
    "4. `load_box_on_truck(\"101\", \"B2\", 5)`\n",
    "5. `load_box_on_truck(\"101\", \"B3\", 5)`\n",
    "6. `load_box_on_truck(\"101\", \"G1\", 15)`\n",
    "7. `calculate_weight_of_truck(\"101\")`\n",
    "8. `prepare_truck(\"102\")`\n",
    "9. `load_box_on_truck(\"102\", \"G2\", 15)`\n",
    "10. `load_box_on_truck(\"102\", \"G3\", 15)`\n",
    "11. `load_box_on_truck(\"102\", \"G4\", 15)`\n",
    "12. `calculate_weight_of_truck(\"102\")`\n",
    "\n",
    "These commands will prepare the trucks, load the boxes, and calculate the weight of each truck as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found OpenAPI Base Endpoint: https://dzgpt4westus.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "if load_dotenv():\n",
    "    print(\"Found OpenAPI Base Endpoint: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"No file .env found\")\n",
    "\n",
    "# Setting up the deployment name\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "\n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "\n",
    "# Currently Chat Completion API have the following versions available: 2023-07-01-preview\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\") \n",
    "\n",
    "from openai import AzureOpenAI\n",
    " \n",
    "client = AzureOpenAI(\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version = os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") # f\"{os.getenv(\"AZURE_OPENAI_ENDPOINT\")}openai/deployments/{os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")}/extensions\" \n",
    "    )\n",
    "\n",
    "def run_conversation(messages, functions, available_functions, deployment_id):\n",
    "    # Step 1: send the conversation and available functions to GPT\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages = messages,\n",
    "        tools = functions,\n",
    "        tool_choice = \"auto\", \n",
    "    )\n",
    "    print(response)\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Step 2: check if GPT wanted to call a function\n",
    "    if tool_calls:\n",
    "        print(\"Recommended Function call:\")\n",
    "        print(tool_calls)\n",
    "        print()\n",
    "    \n",
    "        # Step 3: call the function\n",
    "        messages.append(response_message)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            # verify function exists\n",
    "            if function_name not in available_functions:\n",
    "                return \"Function \" + function_name + \" does not exist\"\n",
    "            else:\n",
    "                print(\"Calling function: \" + function_name)\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(**function_args)\n",
    "            print(\"Function response: \", function_response)\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            ) \n",
    "            \n",
    "        print(\"Addding these message to the next prompt:\")\n",
    "        print(messages)\n",
    "            # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model = deployment_id,\n",
    "            messages = messages)  # get a new response from the model where it can see the function response\n",
    "        return second_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9bWfsLFgtPROjYT3ONCs1Aka9btaj', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_moFosHl5OU1m9cGucbdbAUa9', function=Function(arguments='{\"location\": \"Europe/Berlin\"}', name='get_current_time'), type='function'), ChatCompletionMessageToolCall(id='call_opH9cSVrnPXkFqajfpdRq8Rg', function=Function(arguments='{\"weight\": 25, \"distance\": 100}', name='calculate_travel_time'), type='function')]), content_filter_results={})], created=1718730764, model='gpt-4o-2024-05-13', object='chat.completion', system_fingerprint='fp_abc28019ad', usage=CompletionUsage(completion_tokens=54, prompt_tokens=1360, total_tokens=1414), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n",
      "Recommended Function call:\n",
      "[ChatCompletionMessageToolCall(id='call_moFosHl5OU1m9cGucbdbAUa9', function=Function(arguments='{\"location\": \"Europe/Berlin\"}', name='get_current_time'), type='function'), ChatCompletionMessageToolCall(id='call_opH9cSVrnPXkFqajfpdRq8Rg', function=Function(arguments='{\"weight\": 25, \"distance\": 100}', name='calculate_travel_time'), type='function')]\n",
      "\n",
      "Calling function: get_current_time\n",
      "Function response:  07:12:45 PM\n",
      "Calling function: calculate_travel_time\n",
      "calculate time for traveling for weight:  25  and distance:  100\n",
      "Function response:  1000\n",
      "Addding these message to the next prompt:\n",
      "[{'role': 'system', 'content': '\\n    ##\\n    You are a logistic agent for calculating time for shipping cargo of boxes using a truck. \\n    You need to perform the following tasks based on the User query. \\n    The task aims to create commands and provide the commands as output.\\n    Only one box can be loaded or unloaded at a time.\\n    You should calculate the weight of the truck before every time a box has to be loaded to make sure you do no exceed the maximum weight.\\n    It is most important to not exceed the maximum weight of the truck or the maximum number of boxes that can be loaded onto the truck. Distribute the boxed accordingly and load the heaviest boxes first.\\n\\n    If you are not able to understand the User query. Take a deep breath, think step by step. \\n    Despite deliberation, if you are not able to create commands. Just answer with not able to create commands.\\n    The grammar defines several commands for shipping cargo. Each command takes specific arguments. \\n    The \\'%Y-%m-%d %H:%M:%S\\' means string formatted datetime format.\\n\\n    A blue box weighs 5 kilograms and has dimensions. A red box weighs 10 kilograms. A green box weighs 15 kilograms.\\n    A truck can carry a maximum of 5 boxes of cargo or load a maximum of 50 kilograms.\\n    It takes 3 minutes to load or unload a box onto a truck. Only a single box can be loaded or unloaded at a time.\\n    \\n    Use the tool calculate_travel_time to calculate the traveling time of a truck. The function takes the weight of the cargo in kilograms and the distance in kilometers as arguments.\\n\\n    You are able to create the follwing commands:\\n\\n    The `prepare_truck` command takes new truck and gives it an unique identifier.\\n        `prepare_truck (truck_id)`\\n    \\n    The `load_box_on_truck` command takes a weight of the box as argument and adds a box to the truck. The weight is a number that represents the weight of the cargo in kilograms.\\n        `load_box_on_truck (truck_id, box_id, weight)`\\n\\n    The `calculate_weight_of_truck` command calculated the weight of all the boxes in the truck. The weight is a number that represents the weight of the cargo in kilograms.\\n        `calculate_weight_of_truck (truck_id)`\\n\\n    The `drive_truck_to_location` command takes a weight of the cargo in kilograms and the distance in kilometers. The weight is a number that represents the weight of the cargo in kilograms.\\n        `drive_truck_to_location (truck_id, weight, distance)`\\n    \\n    The `unload_box_from_truck` command takes a weight of the box as argument and unloads a box from the truck. The weight is a number that represents the weight of the cargo in kilograms.\\n        `unload_box_from_truck (truck_id, box_id, weight)`\\n\\n    ## Here are some examples of user inputs that you can use to generate the commands defined by the grammar:\\n\\n    1. For preparing a truck:\\n    \"Please prepare a truck with ID 42.\"\\n\\n    2. For loading a box on a truck:\\n    \"Please load a blue box with ID 123 on the truck with ID 42.\"\\n    \"Please load a red box with ID 43 on the truck with ID 42.\"\\n    \"Please load a red box with ID 44 on the truck with ID 42.\"\\n\\n    3. For calculating the weight of the truck:\\n    \"Please calculate the weight of the truck with ID 42 after loading the boxes.\"\\n    \\n    4. For driving a truck to a location:\\n    \"Please drive truck with ID 42 to the location 100 kilometers away.\"\\n\\n    5. For unloading a box from a truck:\\n    \"Please unload the blue box with ID 123 from the truck with ID 42.\"\\n\\n    Remember to replace the weights, distance, dates, times, and IDs with your actual data. Also update the weight of the truck after every time a box has been loaded or unloaded.\\n    The dates and times should be in the format \\'%Y-%m-%d %H:%M:%S\\'.\\n\\n    ## Here are some examples of how the output might look like based on the functions you provided:\\n\\n    1. For preparing a truck:\\n    `prepare_truck(\"42\")`\\n\\n    2. For loading a box on a truck:\\n    `load_box_on_truck(\"42\", \"123\", 5)`\\n    `load_box_on_truck(\"42\", \"43\", 10)`\\n    `load_box_on_truck(\"42\", \"44\", 10)`\\n\\n    3. For calculating the weight of the truck:\\n    `calculate_weight_of_truck(\"42\")`\\n\\n    4. For driving a truck to a location:\\n    `drive_truck_to_location(\"42\", 25, 100)`\\n\\n    5. For unloading a box from a truck:\\n    `unload_box_from_truck(\"42\", \"123\", 5)`\\n\\n    ## Your response ought to be the command only as follows examples. However, you can prompt for input to provide the command parameters.\\n\\n    1. `prepare_truck(\"42\")`\\n    2. `load_box_on_truck(\"42\", \"123\", 5)`\\n    3. `calculate_weight_of_truck(\"42\")`\\n    4. `drive_truck_to_location(\"42\", 25, 100)`\\n    5. `unload_box_from_truck(\"42\", \"123\", 5)`\\n\\n    ##\\n\\n    \\n    '}, {'role': 'user', 'content': 'I am in Berlin. I have a red box and 3 blue boxes. I need to ship them across 100 kilometers.  What time is it now? The trucks should be loaded now and should be shipping after it has been loaded. Please tell me when will the truck arrive?'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_moFosHl5OU1m9cGucbdbAUa9', function=Function(arguments='{\"location\": \"Europe/Berlin\"}', name='get_current_time'), type='function'), ChatCompletionMessageToolCall(id='call_opH9cSVrnPXkFqajfpdRq8Rg', function=Function(arguments='{\"weight\": 25, \"distance\": 100}', name='calculate_travel_time'), type='function')]), {'tool_call_id': 'call_moFosHl5OU1m9cGucbdbAUa9', 'role': 'tool', 'name': 'get_current_time', 'content': '07:12:45 PM'}, {'tool_call_id': 'call_opH9cSVrnPXkFqajfpdRq8Rg', 'role': 'tool', 'name': 'calculate_travel_time', 'content': 1000}]\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid type for 'messages[4].content': expected one of a string or array of objects, but got an integer instead.\", 'type': 'invalid_request_error', 'param': 'messages[4].content', 'code': 'invalid_type'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m userInput \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am in Berlin. I have a red box and 3 blue boxes. I need to ship them across 100 kilometers.  What time is it now? The trucks should be loaded now and should be shipping after it has been loaded. Please tell me when will the truck arrive?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: commandprompt}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: userInput}]\n\u001b[0;32m----> 4\u001b[0m assistant_response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_functions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeployment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model responds with the function data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(assistant_response\u001b[38;5;241m.\u001b[39mchoices)\n",
      "Cell \u001b[0;32mIn[8], line 82\u001b[0m, in \u001b[0;36mrun_conversation\u001b[0;34m(messages, functions, available_functions, deployment_id)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(messages)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# extend conversation with function response\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m second_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeployment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# get a new response from the model where it can see the function response\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m second_response\n",
      "File \u001b[0;32m/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/openai/resources/chat/completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    605\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/webstreamlit/lib/python3.12/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid type for 'messages[4].content': expected one of a string or array of objects, but got an integer instead.\", 'type': 'invalid_request_error', 'param': 'messages[4].content', 'code': 'invalid_type'}}"
     ]
    }
   ],
   "source": [
    "userInput = \"I am in Berlin. I have a red box and 3 blue boxes. I need to ship them across 100 kilometers.  What time is it now? The trucks should be loaded now and should be shipping after it has been loaded. Please tell me when will the truck arrive?\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": commandprompt}, {\"role\": \"user\", \"content\": userInput}]\n",
    "assistant_response = run_conversation(messages, functions, available_functions, deployment_name)\n",
    "print(\"The model responds with the function data:\")\n",
    "print(assistant_response.choices)\n",
    "print(assistant_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
